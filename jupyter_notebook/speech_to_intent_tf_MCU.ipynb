{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech to Intent Model Training and Conversion to TensorFlow for Microcontrollers\n",
    "This model was developed to run on a Seeedstudio WIO Terminal with a ATSAMD51P19 MCU and ARM Cortex-M4F core\n",
    "The specifications can be found on Seeedstudio's website, current link is posted below:\n",
    "https://www.seeedstudio.com/wio-terminal\n",
    "\n",
    "Before using this notebook, the following packages are necessary and can be installed with pip or conda\n",
    "It is recommended to run these in a virtual environment and/or with conda\n",
    "- pip install pandas\n",
    "- pip install numpy\n",
    "- pip install librosa (for audio and music processing in Python)\n",
    "- pip install audiomentations\n",
    "- pip install tensorflow (various versions might be needed depending on your Python/Anaconda/Jupyter versions)\n",
    "\n",
    "Goals of this project:\n",
    "- As of January 2023, there is still not a considerable amount of support for FOSS speech recognition tasks that are capable of integrating with MCUs\n",
    "- Microcontrollers are cheap and consume low amounts of energy\n",
    "- WIO Terminal can be integrated with SBCs and other IoT devices for a plethora of use cases\n",
    "- The other side of it is I just like playing with fancy software and electronics :)\n",
    "\n",
    "Use Cases Beyond This One:\n",
    "- Audio detection for home security (high decibel levels can be detected, noises can be classified into types (gunshots, glass breaking, dogs barking, etc.)\n",
    "- Home automation - it is possible to control smart light bulbs, thermostats, washers and dryers, door locks, etc with additional hardware (WIO terminal comes with built in Wi-Fi connectivity and Raspberry Pi 40-pin compatibility.)\n",
    "\n",
    "\n",
    "Constraints\n",
    "- Large vocabularies cannot be implemented due to the RAM and flash constraints of MCUs\n",
    "\n",
    "Background Context for Sound Processing \n",
    "- Sound is nothing more than a vibration that propogates through a transmission medium (solid, liquid, gas)\n",
    "- One molecule \"pushes\" another, that molucule pushes another and so on and so forth until it reaches another object\n",
    "- Same principle applies to microphones\n",
    "    -- A microphone has a diaphram that is pushed by sound waves and then returns to its origin position by a magnet\n",
    "    -- This is then converted to an electrical signal (alternating current) which is proportional to sound amplitude (the louder the sound, the more the diaphram is pushed, and more current is produced)\n",
    "    -- We record this with an analog to digital converter and record it in intervals\n",
    "    -- Sampling rate = the number of times a reading is taken in one second - Hz are considered a cycle/second\n",
    "- We can visualize audio signal as a graph of Amplitude (y-axis) vs Time (number of samples)\n",
    "    -- This is not very helpful for analyzing sound\n",
    "    -- Fourier transforms can be applied to decompose a singal into individual frequencies and the frequency's amplitude\n",
    "    -- Multiple Fourier transforms can be applied and appended together to create a Spectrogram (Hz vs. Time) with color coded decibel levels\n",
    "- Humans do not percieve frequencies in the linear scale\n",
    "- We are better at detecting differences in lower frequencies (detecting 500 Hz vs 1000 Hz is easier than detecting 10,000 Hz and 10,500 Hz)\n",
    "- Humans perception of sound range is about 20 Hz to 20,000 Hz\n",
    "- The Mel scale was developed to place more weight on the frequencies that the human ear can hear\n",
    "\n",
    "Overview of this Project\n",
    "- Speech to intent directly converted to speech to parsed intent which is based on a predefined and specific domain vocabulary\n",
    "- i.e. \"Alexa, turn on the lights in the bedroom\" this would get parsed into an output intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "\n",
    "import io, base64\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "\n",
    "import IPython\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Reshape, Flatten, Activation, Add\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax, TimeDistributed, LSTM\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from keras import backend as K \n",
    "\n",
    "import random \n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "import soundfile as sf\n",
    "from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise, PitchShift, Shift, ClippingDistortion, Gain, LoudnessNormalization, TimeStretch \n",
    "from tensorflow.python.ops import gen_audio_ops as contrib_audio\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "project_path = \"../checkpoints\"  # creates directory to save the model\n",
    "data_path = \"../resources/\"  # where I store my dataset\n",
    "\n",
    "train_dataset_path = os.path.join(data_path, 'data/csv/train_data.csv')\n",
    "valid_dataset_path = os.path.join(data_path, 'data/csv/valid_data.csv')\n",
    "#test_dataset_path = os.path.join(data_path, 'data/csv/test_data.csv')\n",
    "test_dataset_path = os.path.join(data_path, 'data/csv/wt_data.csv')\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "MIN_FREQ = 100\n",
    "MAX_FREQ = SAMPLING_RATE//2\n",
    "WIN_SIZE_MS = 0.02\n",
    "WIN_INCREASE_MS = 0.02\n",
    "NUM_CEPSTRAL = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below opens the dataset so we can look at its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_dataset_path)\n",
    "test_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will display the first entry in the wav_file folder so we can ensure it matches the transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = [data_path, \"data\"]\n",
    "wav_file = os.path.join(*prefix, test_data['path'][0])\n",
    "print(wav_file)\n",
    "IPython.display.Audio(wav_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to match the MFCC processing params on our device, so audio_spectrogram and mfcc functions in TF gen_audio_ops. We can use generate_features to create a spectrogram and convert it to mel frequency and visualize it with matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
    "\n",
    "if DEBUG:\n",
    "    print(wav_file)\n",
    "\n",
    "def generate_features(draw_graphs, raw_data, sampling_freq,\n",
    "                      frame_length, frame_stride, num_filters, \n",
    "                      num_cepstral, low_frequency, high_frequency):\n",
    "    graphs = []\n",
    "    \n",
    "    raw_data = np.expand_dims(raw_data, axis = -1)\n",
    "    window_size = int(sampling_freq * frame_length)\n",
    "    stride = int(sampling_freq * frame_stride)\n",
    "    \n",
    "    spectrogram = contrib_audio.audio_spectrogram(\n",
    "        raw_data,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        magnitude_squared=True)\n",
    "    \n",
    "    mfcc = contrib_audio.mfcc(\n",
    "        spectrogram,\n",
    "        sampling_freq,\n",
    "        dct_coefficient_count=num_cepstral,\n",
    "        upper_frequency_limit=high_frequency, \n",
    "        lower_frequency_limit=low_frequency)\n",
    "    \n",
    "    mfcc = np.squeeze(mfcc)\n",
    "\n",
    "    if draw_graphs:\n",
    "        mfcc_graph = np.swapaxes(mfcc, 0, 1)\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(mfcc_graph, x_axis='time', ax=ax)\n",
    "        fig.colorbar(img, ax=ax)\n",
    "        ax.set(title='MFCC')\n",
    "        buf = io.BytesIO()\n",
    "\n",
    "        plt.savefig(buf, format='svg', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        buf.seek(0)\n",
    "        image = (base64.b64encode(buf.getvalue()).decode('ascii'))\n",
    "\n",
    "        buf.close()\n",
    "\n",
    "        graphs.append({\n",
    "            'name': 'Cepstral Coefficients',\n",
    "            'image': image,\n",
    "            'imageMimeType': 'image/svg+xml',\n",
    "            'type': 'image'\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'features': mfcc,\n",
    "        'graphs': graphs,\n",
    "        'output_config': {\n",
    "            'type': 'spectrogram',\n",
    "            'shape': {\n",
    "                'width': mfcc.shape[1],\n",
    "                'height': mfcc.shape[0]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "processed = generate_features(True, audio, SAMPLING_RATE, \n",
    "                              WIN_SIZE_MS, WIN_INCREASE_MS, 32, \n",
    "                              NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
    "\n",
    "if DEBUG:\n",
    "    print(processed['features'])\n",
    "    \n",
    "print(processed['output_config'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proceeding cell we process the .csv file data into labels for the model. Slots are included for objects and locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFactory:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.actions = set()\n",
    "        self.objects = set()\n",
    "        self.locations = set()\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def get_query_slots(self, sentence):\n",
    "\n",
    "        slots = [sentence[0], sentence[1]]\n",
    "        return slots      \n",
    "    \n",
    "    def get_properties(self, data):\n",
    "\n",
    "        data[\"action\"] = data['action'].str.lower()\n",
    "        data[\"object\"] = data['object'].str.lower()\n",
    "        data[\"location\"] = data['location'].str.lower()\n",
    "\n",
    "        actions = set(data.action.unique())\n",
    "        objects = set(data.object.unique())\n",
    "        locations = set(data.location.unique())\n",
    "\n",
    "        return actions, objects, locations        \n",
    "\n",
    "    def get_vocab(self, actions, objects, locations, data):\n",
    "\n",
    "        vocab = objects | locations\n",
    "\n",
    "        if DEBUG:\n",
    "            print(vocab)\n",
    "\n",
    "        data[\"transcription\"] = data['transcription'].str.replace('[^\\w\\s]','')\n",
    "        data[\"transcription\"] = data['transcription'].str.lower()\n",
    "\n",
    "        for item in data.transcription:\n",
    "            for word in item.split(\" \"):\n",
    "                vocab.add(word)\n",
    "\n",
    "        vocab = [s.strip() for s in vocab]\n",
    "        \n",
    "        return set(vocab)   \n",
    "    \n",
    "    def add_corpora(self, data):\n",
    "        \n",
    "        actions, objects, locations = self.get_properties(data)\n",
    "        vocab = self.get_vocab(actions, objects, locations, data)\n",
    "\n",
    "        self.actions = set(self.actions | actions)\n",
    "        self.objects = set(self.objects | objects)\n",
    "        self.locations = set(self.locations | locations)        \n",
    "        self.vocab = set(self.vocab | vocab)  \n",
    "        self.query_slots = set(self.objects | self.locations)\n",
    "        \n",
    "    def process_data(self, data):\n",
    "        \n",
    "        self.actions = list(self.actions)\n",
    "        self.objects = list(self.objects)\n",
    "        self.locations = list(self.locations)       \n",
    "        self.vocab = list(self.vocab)\n",
    "        self.query_slots = list(self.query_slots)\n",
    "        \n",
    "        word_ids, slot_ids, intent_ids = {' ': 0}, {}, {self.actions[i]: i for i in range(0, len(self.actions))}\n",
    "\n",
    "        slots = []\n",
    "        for sentence in zip(data.object, data.location):\n",
    "            slots.append(self.get_query_slots(sentence))\n",
    "        \n",
    "        i = 0\n",
    "        for slot in self.query_slots:\n",
    "            if slot == 'none':\n",
    "                continue\n",
    "            slot_ids[slot] = i\n",
    "            i += 1\n",
    "            \n",
    "        slot_ids['none'] = i\n",
    "\n",
    "        #convert vocab to dictionary\n",
    "        start = 1\n",
    "        for i in range(len(self.vocab)):\n",
    "            word_ids[self.vocab[i]] = start + i\n",
    "        word_ids['unknown'] =  i + 1  \n",
    "\n",
    "        #create reverse dicts\n",
    "        ids2words = dict((v, k) for k, v in word_ids.items())\n",
    "        ids2slots = dict((v, k) for k, v in slot_ids.items())\n",
    "        ids2intents = dict((v, k) for k, v in intent_ids.items())\n",
    "\n",
    "        n_vocab = len(ids2words)\n",
    "\n",
    "        n_classes = len(ids2intents)\n",
    "        n_slots = len(ids2slots)\n",
    "\n",
    "        vectorized_slots = list(map(lambda slots: np.array(list(map(lambda slot: slot_ids[slot], slots))), slots))\n",
    "        vectorized_intents = list(map(lambda l: np.array([intent_ids[l]]), data.action))\n",
    "\n",
    "        filepaths = data['path'].to_numpy()\n",
    "\n",
    "        return ids2intents, ids2slots, vectorized_slots, vectorized_intents, filepaths        \n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open(os.path.join(data_path, 'data/pkl/'+ name + '.pkl'), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(os.path.join(data_path, 'data/pkl/'+ name + '.pkl'), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "generate_data = True #change that to True the first time you are running the code\n",
    "\n",
    "train_data = pd.read_csv(train_dataset_path)\n",
    "valid_data = pd.read_csv(valid_dataset_path)\n",
    "test_data = pd.read_csv(test_dataset_path)\n",
    "\n",
    "if generate_data:\n",
    "    \n",
    "    dataset_processor = DatasetFactory()\n",
    "    \n",
    "    train_data = pd.read_csv(train_dataset_path)\n",
    "    valid_data = pd.read_csv(valid_dataset_path)\n",
    "    test_data = pd.read_csv(test_dataset_path)\n",
    "    \n",
    "    dataset_processor.add_corpora(train_data)\n",
    "    dataset_processor.add_corpora(valid_data)\n",
    "    dataset_processor.add_corpora(test_data)\n",
    "    \n",
    "    ids2intents, ids2slots, vectorized_slots_train, vectorized_intents_train, filepaths_train = dataset_processor.process_data(train_data)\n",
    "    _ids2intents, _ids2slots, vectorized_slots_valid, vectorized_intents_valid, filepaths_valid = dataset_processor.process_data(valid_data)\n",
    "    __ids2intents, __ids2slots, vectorized_slots_test, vectorized_intents_test, filepaths_test = dataset_processor.process_data(test_data)\n",
    "\n",
    "    assert ids2intents == _ids2intents == __ids2intents\n",
    "    assert ids2slots == _ids2slots == __ids2slots\n",
    "    \n",
    "    save_obj(ids2intents, 'ids2intents')\n",
    "    save_obj(ids2slots, 'ids2slots')\n",
    "    \n",
    "    save_obj(vectorized_slots_train, 'vectorized_slots_train')\n",
    "    save_obj(vectorized_intents_train, 'vectorized_intents_train')\n",
    "    \n",
    "    save_obj(vectorized_slots_valid, 'vectorized_slots_valid')\n",
    "    save_obj(vectorized_intents_valid, 'vectorized_intents_valid')\n",
    "    \n",
    "    save_obj(vectorized_slots_test, 'vectorized_slots_test')\n",
    "    save_obj(vectorized_intents_test, 'vectorized_intents_test') \n",
    "    \n",
    "else:\n",
    "\n",
    "    filepaths_train = train_data['path'].to_numpy()\n",
    "    filepaths_valid = valid_data['path'].to_numpy()\n",
    "    filepaths_test = test_data['path'].to_numpy()\n",
    "    \n",
    "    ids2intents = load_obj('ids2intents')\n",
    "    ids2slots = load_obj('ids2slots')\n",
    "    \n",
    "    vectorized_slots_train = load_obj('vectorized_slots_train')\n",
    "    vectorized_intents_train = load_obj('vectorized_intents_train')\n",
    "    \n",
    "    vectorized_slots_valid = load_obj('vectorized_slots_valid')\n",
    "    vectorized_intents_valid = load_obj('vectorized_intents_valid')\n",
    "    \n",
    "    vectorized_slots_test = load_obj('vectorized_slots_test')\n",
    "    vectorized_intents_test = load_obj('vectorized_intents_test')\n",
    "    \n",
    "if DEBUG:\n",
    "    print(vectorized_slots_test)\n",
    "    print(vectorized_intents_test)\n",
    "    print(ids2intents) \n",
    "    print(ids2slots) \n",
    "\n",
    "print(str(ids2intents.values()).replace(\"'\", \"\\\"\")) \n",
    "print(str(ids2slots.values()).replace(\"'\", \"\\\"\")) \n",
    "\n",
    "n_classes = len(ids2intents)\n",
    "n_slots = len(ids2slots)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell generates a data generator class and instantializes it. The training data was from TinyML's Fluent Speech Commands Dataset and is slightly altered to include background noise to random samples using AddGausianNoise, AddBackgroundNoise, and ClippingDistortion. TinyMl's dataset contains 97 speakers saying 248 different phrases. The utterances are mapped to 31 unique intents which are divided into three slots: action, object, and location. The dataset also includes non native english speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aug_pipeline():\n",
    "    \n",
    "    aug_pipeline = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.1),\n",
    "    AddBackgroundNoise(sounds_path=os.path.join(data_path, \"data/wavs/background_noise\"), p=0.3),\n",
    "    ClippingDistortion(p=0.3),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.2),\n",
    "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.1),\n",
    "    Gain(p=0.2),\n",
    "    TimeStretch(p=0.05)\n",
    "    ])\n",
    "    \n",
    "    return aug_pipeline\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, entries, num_list, batch_size, shuffle=True, to_fit=True, augment = True, vis = False):\n",
    "\n",
    "        self.entries = entries\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.n_intents, self.n_slots = num_list\n",
    "        \n",
    "        self.len = 2\n",
    "        self.aug_pipeline = None\n",
    "        if augment:\n",
    "            self.aug_pipeline = create_aug_pipeline()\n",
    "        self.vis = vis\n",
    "        self.shuffle = shuffle\n",
    "        self.to_fit = to_fit\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.entries[0]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        X_batch = [self.entries[0][k] for k in indexes]\n",
    "        \n",
    "        Y_intent = [self.entries[1][k] for k in indexes]\n",
    "        Y_slot = [self.entries[2][k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X = self._generate_X(X_batch)\n",
    "\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(Y_intent, Y_slot)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.entries[0]))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _generate_X(self, batch_items):\n",
    "\n",
    "        X = np.zeros(shape = (self.batch_size, 150, NUM_CEPSTRAL, 1))\n",
    "         \n",
    "        for i, batch_item in enumerate(batch_items):\n",
    "            wav_file = os.path.join(*prefix, batch_item)\n",
    "            audio, sample_rate = librosa.load(os.path.join(wav_file), sr=16000, res_type='kaiser_best')\n",
    "            audio = librosa.util.fix_length(audio, 16000*3)\n",
    "            \n",
    "            if self.aug_pipeline:\n",
    "                audio = self.aug_pipeline(audio, sample_rate)\n",
    "                \n",
    "                if DEBUG:\n",
    "                    new_filename = os.path.join('samples', os.path.basename(batch_item.split('.')[0]+'aug.wav'))\n",
    "                    print(\"Augmented: \", new_filename)\n",
    "                    print(\"--------------\")\n",
    "                    sf.write(new_filename, audio, sample_rate,  subtype='PCM_16')\n",
    "                \n",
    "            output = generate_features(self.vis, audio, SAMPLING_RATE, \n",
    "                                          WIN_SIZE_MS, WIN_INCREASE_MS, 32, \n",
    "                                          NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
    "\n",
    "            features = output['features']\n",
    "            X[i, ] = np.expand_dims(features, axis = -1)\n",
    "        return X\n",
    "    \n",
    "    def _generate_y(self, intents, slots):\n",
    "        intent_y = np.empty((self.batch_size, self.n_intents), dtype=int)\n",
    "        slot_y = np.empty((self.batch_size, self.len, self.n_slots), dtype=int)      \n",
    "\n",
    "        # Generate data\n",
    "        for i, batch_item in enumerate(intents):\n",
    "            intent = intents[i]\n",
    "            slot = slots[i]\n",
    "            intent_y[i,] = np.eye(self.n_intents)[intent]\n",
    "            slot_y[i,] = np.eye(self.n_slots)[slot][np.newaxis, :]\n",
    "        \n",
    "        return [intent_y, slot_y]\n",
    "\n",
    "batch_size = 32   \n",
    "    \n",
    "training_generator = DataGenerator([filepaths_train, vectorized_intents_train, vectorized_slots_train], \n",
    "                                   [n_classes,n_slots], batch_size = batch_size, \n",
    "                                   shuffle=True, to_fit=True, augment = True)\n",
    "\n",
    "data = training_generator.__getitem__(0)\n",
    "print(data[0].shape)\n",
    "print(data[1][0].shape)\n",
    "print(data[1][1].shape)\n",
    "print(training_generator.__len__())\n",
    "\n",
    "validation_generator = DataGenerator([filepaths_valid, vectorized_intents_valid, vectorized_slots_valid], \n",
    "                                     [n_classes,n_slots], batch_size = batch_size, \n",
    "                                     shuffle=False, to_fit=True, augment = False)\n",
    "\n",
    "\n",
    "data = validation_generator.__getitem__(0)\n",
    "print(data[0].shape)\n",
    "print(data[1][0].shape)\n",
    "print(data[1][1].shape)\n",
    "print(validation_generator.__len__())\n",
    "\n",
    "test_generator = DataGenerator([filepaths_test, vectorized_intents_test, vectorized_slots_test], \n",
    "                                     [n_classes, n_slots], batch_size = batch_size, vis = False,\n",
    "                                     shuffle=False, to_fit=True, augment = False)\n",
    "\n",
    "data = test_generator.__getitem__(0)\n",
    "print(data[0].shape)\n",
    "print(data[1][0].shape)\n",
    "print(data[1][1].shape)\n",
    "print(test_generator.__len__())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell generates model archicture, and is simply 2D Convolution layers with Batch Normalization and Max Pooling 2D layers. Last, Global Max Pooling is implemented and features are fed into Dense layer which then are mapped to slot and intent outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
    "\n",
    "x = Conv2D(16, 3, padding='same', activation='relu', use_bias = False)(main_input)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "x = Conv2D(32, 3, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "x = Conv2D(128, 2, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "slot_dense = Dense(n_slots*2)(x)\n",
    "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
    "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
    "\n",
    "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
    "\n",
    "optim = Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very much possible to achieve a slot output accuracy in the 90s and an intent accuracy in the 80s and 90s. The number of epochs is a choice the user can make, but I would suggest at least 30 and an accuracy at the very least above 50%. Additionally, the epochs will run much faster if you are able to use TensorFlow connected to your computer's GPU. It is also possible to run this in Google Collab if hardware resources are limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(output_path)\n",
    "print(\"Project folder: {}\".format(output_path))\n",
    "\n",
    "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
    "log_dir =  os.path.join(output_path, \"logs\")\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
    "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
    "    TensorBoard(log_dir=log_dir),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
    "try:\n",
    "    model.fit(training_generator, validation_data = test_generator,\n",
    "              callbacks = my_callbacks, epochs = 70,\n",
    "              workers = 4, max_queue_size = 10,\n",
    "              use_multiprocessing = False)\n",
    "except KeyboardInterrupt:\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Layer\n",
    "The baseline model falls short in that it does not preserve the temporal dependencies in the outut of the feature extractor. To preserve the temporal component of the signal, we can utilize a singular LSTM later after Conv 2D wrapped with a TimeDistributed layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
    "\n",
    "x = Conv2D(16, 3, padding='same', activation='relu', use_bias = False)(main_input)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "x = Conv2D(16, 2, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "x = Conv2D(32, 3, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "x = Conv2D(128, 2, padding='same', activation='relu', use_bias = False)(x)\n",
    "x = TimeDistributed(Flatten())(x)\n",
    "\n",
    "x = LSTM(32, activation='relu')(x)\n",
    "\n",
    "slot_dense = Dense(n_slots*2)(x)\n",
    "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
    "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
    "\n",
    "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
    "\n",
    "optim = Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(output_path)\n",
    "print(\"Project folder: {}\".format(output_path))\n",
    "\n",
    "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
    "log_dir =  os.path.join(output_path, \"logs\")\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
    "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
    "    TensorBoard(log_dir=log_dir),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
    "try:\n",
    "    model.fit(training_generator, validation_data = test_generator,\n",
    "              callbacks = my_callbacks, epochs = 30,\n",
    "              workers = 4, max_queue_size = 10,\n",
    "              use_multiprocessing = False)\n",
    "except KeyboardInterrupt:\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Connections\n",
    "Another way to improve the quality of the network predictions is to add skip connections by implementing ResNet blocks with the intent being that the low level feature info will be passed on to the top levels of the feature extractor and used in the prediction making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_residual_block(X, num_channels, use_1x1conv=False, strides=1):\n",
    "        conv1 = Conv2D(num_channels, padding='same',\n",
    "                                            kernel_size=3, strides=strides)\n",
    "        conv2 = Conv2D(num_channels, kernel_size=3,\n",
    "                                            padding='same')\n",
    "        conv3 = None\n",
    "        if use_1x1conv:\n",
    "            conv3 = Conv2D(num_channels, kernel_size=1,\n",
    "                                                strides=strides)\n",
    "        bn1 = BatchNormalization()\n",
    "        bn2 = BatchNormalization()\n",
    "        relu1 = Activation(relu)\n",
    "        relu2 = Activation(relu)\n",
    "        Y = relu1((bn1(conv1(X))))\n",
    "        Y = bn2(conv2(Y))\n",
    "        if conv3 is not None:\n",
    "            X = conv3(X)\n",
    "        Y = Add()([Y, X])\n",
    "        return relu2(Y) \n",
    "def make_resnet_block(x, num_channels, num_residuals, first_block=False):\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            x = make_residual_block(x, num_channels, use_1x1conv=True, strides=2)\n",
    "        else:\n",
    "            x = make_residual_block(x, num_channels)\n",
    "    return x\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
    "\n",
    "x = make_resnet_block(main_input, 8, 1, first_block=True)\n",
    "x = make_resnet_block(x, 16, 2, first_block=False)\n",
    "x = make_resnet_block(x, 32, 3, first_block=False)\n",
    "x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "slot_dense = Dense(n_slots*2)(x)\n",
    "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
    "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
    "\n",
    "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
    "\n",
    "optim = Adam(learning_rate=1e-3, decay=1e-6)\n",
    "\n",
    "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(output_path)\n",
    "print(\"Project folder: {}\".format(output_path))\n",
    "\n",
    "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
    "log_dir =  os.path.join(output_path, \"logs\")\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
    "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
    "    TensorBoard(log_dir=log_dir),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
    "try:\n",
    "    model.fit(training_generator, validation_data = test_generator,\n",
    "              callbacks = my_callbacks, epochs = 30,\n",
    "              workers = 4, max_queue_size = 10,\n",
    "              use_multiprocessing = False)\n",
    "except KeyboardInterrupt:\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depthwise Convolutions\n",
    "Depthwise convolution blocks facilitate less multiplication operations by following it with pointwise convolution. We can use this to apply a single convolution filter to each input channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_conv_block(inputs, num_filters = 16, alpha = 1, kernel_size = 2, pooling = None, block_id=1, activation = 'relu'):\n",
    "\n",
    "    x = Conv2D(int(num_filters*alpha), kernel_size, padding='same', use_bias = False, name='conv_%d' % block_id)(inputs)\n",
    "    x = BatchNormalization(name='conv_%d_bn' % block_id)(x)\n",
    "    x = Activation(activation, name='conv_%d_act' % block_id)(x)\n",
    "\n",
    "    if pooling:\n",
    "        x = MaxPooling2D(pool_size = pooling, name='conv_%d_pool' % block_id)(x)\n",
    "    return x\n",
    "    \n",
    "def dw_conv_block(inputs, num_filters, alpha, depth_multiplier=1, strides=(1, 1), block_id=1, activation = 'relu'):\n",
    "\n",
    "    pointwise_conv_filters = int(num_filters * alpha)\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = ZeroPadding2D(((0, 1), (0, 1)),\n",
    "                                 name='conv_pad_%d' % block_id)(inputs)\n",
    "    x = DepthwiseConv2D((2, 2),\n",
    "                               padding='same' if strides == (1, 1) else 'valid',\n",
    "                               depth_multiplier=depth_multiplier,\n",
    "                               #strides=strides,\n",
    "                               use_bias=False,\n",
    "                               name='conv_dw_%d' % block_id)(x)\n",
    "    x = BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = Activation(activation, name='conv_dw_%d_act' % block_id)(x)\n",
    "\n",
    "    x = Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      strides=(1, 1),\n",
    "                      name='conv_pw_%d' % block_id)(x)\n",
    "    x = BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    x = Activation(activation, name='conv_pw_%d_act' % block_id)(x)\n",
    "\n",
    "    if strides > 1:\n",
    "        x = MaxPooling2D(pool_size = 2, name='conv_%d_pool' % block_id)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _depth(v, divisor=8, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "main_input = Input(shape=(150, NUM_CEPSTRAL, 1), name='main_input')\n",
    "\n",
    "x = plain_conv_block(main_input, num_filters = 16, alpha = 1, kernel_size = 2, pooling = None, block_id=0, activation = 'relu')\n",
    "\n",
    "x = dw_conv_block(x, 16, 2, depth_multiplier=1, strides=1, block_id=1, activation = 'relu')\n",
    "x = dw_conv_block(x, 16, 2, depth_multiplier=1, strides=2, block_id=2, activation = 'relu')\n",
    "x = dw_conv_block(x, 16, 2, depth_multiplier=1, strides=1, block_id=3, activation = 'relu')\n",
    "x = dw_conv_block(x, 32, 2, depth_multiplier=1, strides=2, block_id=4, activation = 'relu')\n",
    "x = dw_conv_block(x, 128, 2, depth_multiplier=1, strides=2, block_id=5, activation = 'relu')\n",
    "\n",
    "x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "slot_dense = Dense(n_slots*2)(x)\n",
    "slot_reshape = Reshape(target_shape = (2, n_slots))(slot_dense)\n",
    "slot_output = Softmax(name='slot_output')(slot_reshape)\n",
    "\n",
    "intent_output = Dense(n_classes, activation='softmax', name='intent_output', use_bias = False)(x)\n",
    "\n",
    "model = Model(inputs=main_input, outputs=[intent_output, slot_output])\n",
    "\n",
    "optim = Adam(learning_rate=1e-3, decay=1e-6)\n",
    "\n",
    "model.compile(optimizer = optim, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='img.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(project_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(output_path)\n",
    "print(\"Project folder: {}\".format(output_path))\n",
    "\n",
    "model_name = os.path.join(output_path, \"slu_model.h5\")\n",
    "log_dir =  os.path.join(output_path, \"logs\")\n",
    "\n",
    "my_callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose = 1),\n",
    "    ModelCheckpoint(filepath=model_name, save_best_only=True, verbose = 1),\n",
    "    TensorBoard(log_dir=log_dir),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose = 1)]\n",
    "try:\n",
    "    model.fit(training_generator, validation_data = test_generator,\n",
    "              callbacks = my_callbacks, epochs = 30,\n",
    "              workers = 4, max_queue_size = 10,\n",
    "              use_multiprocessing = False)\n",
    "except KeyboardInterrupt:\n",
    "    raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Complete\n",
    "Now we can check the model accuracy and display random sample inference results. Further, using test_models(model_directory=\"checkpoints\") allows us to test all the models in the experiment folder and display names, results, and model summaries. We can then determine which has the higher slot and intent accuracy if it is higher than our pre-set threshold (0.8 for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_threshold = 0.8\n",
    "\n",
    "def test_models(model_name = None, model_directory = None):\n",
    "    \n",
    "    print(\"Testing\")\n",
    "    if model_directory:\n",
    "        model_files_list = []\n",
    "        file_search = lambda ext : glob.glob(model_directory + ext, recursive=True)\n",
    "        for ext in ['/**/*.h5']: model_files_list.extend(file_search(ext))\n",
    "    else:\n",
    "        model_files_list = [model_name]\n",
    "        \n",
    "    batch_size = 1\n",
    "\n",
    "    test_generator = DataGenerator([filepaths_test, vectorized_intents_test, vectorized_slots_test], \n",
    "                                         [n_classes, n_slots], batch_size = batch_size, vis = False,\n",
    "                                         shuffle=False, to_fit=True, augment = False)\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for model_file in model_files_list:\n",
    "\n",
    "        model = load_model(model_file)\n",
    "      \n",
    "        intent_correct = 0\n",
    "        slot_correct = 0\n",
    "\n",
    "        for num in range(32):\n",
    "\n",
    "            X, y = test_generator.__getitem__(num)\n",
    "\n",
    "            try:\n",
    "                results = model(X, training=False)\n",
    "            except Exception as e:\n",
    "                print('Error')\n",
    "                break\n",
    "\n",
    "            if ids2intents[np.argmax(y[0])] == ids2intents[np.argmax(results[0])]:\n",
    "                intent_correct += 1\n",
    "\n",
    "            if ids2slots[np.argmax(y[1][0][0])] == ids2slots[np.argmax(results[1][0][0])]:\n",
    "                slot_correct += 1 \n",
    "\n",
    "            if ids2slots[np.argmax(y[1][0][1])] == ids2slots[np.argmax(results[1][0][1])]:\n",
    "                slot_correct += 1     \n",
    "\n",
    "        accuracy_intent = intent_correct/32\n",
    "        accuracy_slot = slot_correct/64\n",
    "\n",
    "        if accuracy_intent < accuracy_threshold or accuracy_slot < accuracy_threshold and model_directory:\n",
    "            continue\n",
    "            \n",
    "        #model.summary()    \n",
    "        num = random.randint(0, len(test_generator)-1)\n",
    "\n",
    "        X, y = test_generator.__getitem__(num)\n",
    "        \n",
    "        try:\n",
    "            results = model(X, training=False)\n",
    "        except Exception as e:\n",
    "            print('Error')\n",
    "        \n",
    "        print(f\"\"\"Model {model_file}\n",
    "        \n",
    "        Accuracy Intent {accuracy_intent} %\n",
    "        Accuracy Slot {accuracy_slot} %\n",
    "\n",
    "        Random sample num:{num} \n",
    "\n",
    "        Ground truth \n",
    "        Intent:{ids2intents[np.argmax(y[0])]} \n",
    "        Slot1: {ids2slots[np.argmax(y[1][0][0])]}  Slot2: {ids2slots[np.argmax(y[1][0][1])]}\\n\n",
    "\n",
    "        Prediction\n",
    "        Intent:{ids2intents[np.argmax(results[0])]} \n",
    "        Slot1: {ids2slots[np.argmax(results[1][0][0])]}  Slot2: {ids2slots[np.argmax(results[1][0][1])]}\\n\n",
    "        \"\"\")\n",
    "\n",
    "        if (accuracy_intent + accuracy_slot) / 2 > best_accuracy:\n",
    "            best_model = model\n",
    "            best_model_file = model_file\n",
    "            best_accuracy = (accuracy_intent + accuracy_slot) / 2\n",
    "    return best_model, best_model_file, best_accuracy\n",
    "\n",
    "#model = test_models(model_name = model_name) \n",
    "model, model_name, accuracy = test_models(model_directory = project_path)\n",
    "print(f\"\"\"----------------------------\n",
    "Best model is {model_name} Accuracy {accuracy} %\n",
    "----------------------------\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check_data_prefix = [data_path, \"data\", \"wavs\", \"wt_test\"]\n",
    "\n",
    "wav_file = os.path.join(*sanity_check_data_prefix, \"change_language_to_chinese_wt.wav\")\n",
    "#wav_file = os.path.join(*sanity_check_data_prefix, \"decrease_volume_wt.wav\")\n",
    "#wav_file = os.path.join(*sanity_check_data_prefix, \"turn_on_the_lights_in_the_kitchen_wt.wav\"\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "sample_rate, audio = wav.read(wav_file)\n",
    "if DEBUG:\n",
    "    print(','.join(str(e) for e in audio.tolist()[:4095]))\n",
    "\n",
    "audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
    "audio = librosa.util.fix_length(audio, 16000*3)\n",
    "features = generate_features(True, audio, SAMPLING_RATE, \n",
    "                  WIN_SIZE_MS, WIN_INCREASE_MS, 32, \n",
    "                  NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
    "\n",
    "features = features['features']\n",
    "X = np.expand_dims(features, axis = 0)\n",
    "\n",
    "results = model(X, training=False)\n",
    "print(np.argmax(results[0]), np.argmax(results[1][0][0]), np.argmax(results[1][0][1]))\n",
    "print(f\"\"\"\n",
    "Prediction\n",
    "Intent:{ids2intents[np.argmax(results[0])]} \n",
    "Slot1: {ids2slots[np.argmax(results[1][0][0])]}  Slot2: {ids2slots[np.argmax(results[1][0][1])]}\\n\n",
    "\"\"\")\n",
    "\n",
    "IPython.display.Audio(wav_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to .tflite format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for i in range(len(test_data)):\n",
    "        wav_file = os.path.join(*prefix, test_data['path'][i])\n",
    "        audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
    "        audio = librosa.util.fix_length(audio, 16000*3)\n",
    "        features = generate_features(False, audio, SAMPLING_RATE, \n",
    "                          WIN_SIZE_MS, WIN_INCREASE_MS, 32, \n",
    "                          NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
    "        \n",
    "        features = features['features']\n",
    "        X = np.expand_dims(features, axis = -1)\n",
    "        X = np.expand_dims(X, axis = 0)\n",
    "        yield [X.astype(np.float32)]\n",
    "\n",
    "#model = tf.keras.models.load_model(model_name)\n",
    "model.input.set_shape(1 + model.input.shape[1:])\n",
    "            \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.experimental_new_converter = True\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "converter.inference_type = tf.int8\n",
    "converter.inference_input_type = tf.int8 \n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "tflite_filename = os.path.abspath(model_name).split('.')[0] + '.tflite'\n",
    "with open(tflite_filename, 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare INT8 results with FLOAT32 model and note differences in model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = tflite_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "sanity_check_data_prefix = [*prefix, \"wavs\", \"wt_test\"]\n",
    "\n",
    "wav_file = os.path.join(*sanity_check_data_prefix, \"change_language_to_chinese_wt.wav\")\n",
    "#wav_file = os.path.join(*sanity_check_data_prefix, \"decrease_volume_wt.wav\")\n",
    "#wav_file = os.path.join(*sanity_check_data_prefix, \"turn_on_the_lights_in_the_kitchen_wt.wav\"\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
    "    \n",
    "audio, sample_rate = librosa.load(wav_file, sr=16000, res_type='kaiser_best')\n",
    "audio = librosa.util.fix_length(audio, 16000*3)\n",
    "features = generate_features(True, audio, SAMPLING_RATE, \n",
    "                  WIN_SIZE_MS, WIN_INCREASE_MS, 32, \n",
    "                  NUM_CEPSTRAL, MIN_FREQ, MAX_FREQ)\n",
    "\n",
    "features = features['features']\n",
    "\n",
    "X = np.expand_dims(features, axis = -1)\n",
    "X = np.expand_dims(X, axis = 0)\n",
    "\n",
    "input_data = np.asarray(X, dtype=np.float32)\n",
    "\n",
    "input_data_int8 = np.asarray(input_data/input_scale + input_zero_point, dtype=np.int8)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data_int8)\n",
    "interpreter.invoke()\n",
    "\n",
    "output_data_slot = np.asarray(interpreter.get_tensor(output_details[0]['index']), dtype=np.float32)\n",
    "output_data_intent = np.asarray(interpreter.get_tensor(output_details[1]['index']), dtype=np.float32)\n",
    "\n",
    "intent = (output_data_intent - output_zero_point) * output_scale\n",
    "slot = (output_data_slot - output_zero_point) * output_scale\n",
    "\n",
    "if DEBUG:\n",
    "    print(features)\n",
    "    print(np.argmax(intent[0]), np.argmax(slot[0][0]), np.argmax(slot[0][1]))\n",
    "    \n",
    "print(f\"\"\"\n",
    "Prediction\n",
    "Intent:{ids2intents[np.argmax(intent[0])]} \n",
    "Slot1: {ids2slots[np.argmax(slot[0][0])]}  Slot2: {ids2slots[np.argmax(slot[0][1])]}\\n\n",
    "\"\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hex dump of the .tflite model weights to C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmicro_filename = tflite_filename.split('.')[0] + '.h'\n",
    "!xxd -i $tflite_filename > $tfmicro_filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "293803cbaa357c9860d68c02f93ff4620a4e56c52c7c4c94fc6d47d407649920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
